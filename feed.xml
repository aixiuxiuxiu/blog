<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://aixiuxiuxiu.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aixiuxiuxiu.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-10T16:12:37+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/feed.xml</id><title type="html">blank</title><subtitle>My name is Aixiu. I write here my learning notes for NLP and machine learning. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Optimizing PDF Extraction for Building a Robust RAG</title><link href="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/" rel="alternate" type="text/html" title="Optimizing PDF Extraction for Building a Robust RAG"/><published>2024-08-27T00:32:13+00:00</published><updated>2024-08-27T00:32:13+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/2024/pdfextractor</id><content type="html" xml:base="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/"><![CDATA[<p>Recently, Retrieval-Augmented Generation (RAG) has emerged as a prominent approach that leverages large language models for building advanced applications. However, in practical industrial settings, the primary bottleneck affecting RAG performance—especially in document retrieval—often lies not in the capabilities of the embedding model, but in the data ingestion pipeline. The process of building a RAG system begins with indexing documents, many of which are in PDF format. This typically involves using PDF parsers to extract text from the document’s pages, a crucial step that significantly impacts the overall retrieval accuracy.</p> <p>Extracting text from PDFs are challenging in many aspects (can also read <a href="https://pypdf.readthedocs.io/en/stable/user/extract-text.html">this</a>):</p> <ul> <li><strong>Complex and variable structures</strong>: PDFs are designed for visual presentation, not structured text extraction, leading to fragmented or misaligned text.</li> <li><strong>Layout complexity</strong>: PDFs often contain multi-column formats, tables, and embedded images, making it difficult to maintain a logical reading order.</li> <li><strong>Inconsistent text encoding</strong>: Different fonts, character encodings, or special symbols can lead to extraction errors, such as missing or garbled text.</li> <li><strong>Discontinuous text chunks</strong>: Text might be broken into fragments that need to be reassembled into meaningful sequences.</li> </ul> <p>There are various PDF extraction tools available on the market. Some companies provide paid solutions with advanced features, while there also exists several open-source Python packages, such as <code class="language-plaintext highlighter-rouge">PyPDF</code>, <code class="language-plaintext highlighter-rouge">PDFPlumber</code>, etc. In this discussion, I will compare the impact of different PDF extraction methods on retrieval performance.</p> <h2 id="pdf-extraction">PDF extraction</h2> <p>The following example is a PDF excerpt from the <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a>. The PDF page presents a high level of complexity, with text that is discontinuously arranged and interspersed with numerous footnotes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/pdf_example-480.webp 480w,/blog/assets/img/pdf_example-800.webp 800w,/blog/assets/img/pdf_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/pdf_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here I use <a href="https://github.com/jsvine/pdfplumber"><code class="language-plaintext highlighter-rouge">pdfplumber</code></a>, a library built on top of <a href="https://github.com/goulu/pdfminer"><code class="language-plaintext highlighter-rouge">pdfminer.six</code></a>, that offers a wide range of customizable features for extracting text from PDFs. This package allows the extraction of pages and text while preserving the original layout. Additionally, it can parse various character properties, such as page number, text, and coordinates. For instance, the <code class="language-plaintext highlighter-rouge">.crop()</code> method can be used to crop a page into a specific bounding box: <code class="language-plaintext highlighter-rouge">.crop((x0, top, x1, bottom), relative=False, strict=True)</code>.</p> <p>When I extract text directly from this area, the raw extraction results look unstructured and illogical. It reads the line horizontally, even though the line is split into two blocks.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber_old.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>In another approach, I extract text from two distinct boxes: one for the left side and one for the right side of the page. I use the <code class="language-plaintext highlighter-rouge">x0</code> of the word <strong>Art</strong> as the <code class="language-plaintext highlighter-rouge">x0</code> for the right box and the same value as the <code class="language-plaintext highlighter-rouge">x1</code> for the left box. This makes the extracted sequences more logical.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <h2 id="evaluation">Evaluation</h2> <p>I built two data loaders: one called <code class="language-plaintext highlighter-rouge">PDFLoader</code>, which extracts the sequences directly using <code class="language-plaintext highlighter-rouge">pdfplumber</code> without additional postprocessing, and the other called <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code>, which uses <code class="language-plaintext highlighter-rouge">pdfplumber</code> to combine discontinuous chunks into continuous sequences, as illustrated above. You can find the code on <a href="https://github.com/aixiuxiuxiu/pdf-extraction-blog/tree/main">Github</a>.</p> <p>Here, I will use the <code class="language-plaintext highlighter-rouge">RetrieverEvaluator</code> module provided in <code class="language-plaintext highlighter-rouge">LLAMAIndex</code> to evaluate retrieval quality by comparing these two methods of PDF extraction: <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code> and <code class="language-plaintext highlighter-rouge">PDFLoader</code>. I will use a <code class="language-plaintext highlighter-rouge">top-2 retriever</code> for this evaluation.</p> <ul> <li>First, I used the <code class="language-plaintext highlighter-rouge">generate_question_context_pairs</code> function to auto-generate a set of (question, context) pairs over the entire PDF file <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a>, which contains about 350 pages. I generated 2 questions from each context chunk, resulting in a total number of questions.</li> <li> <p>Then, I ran the RetrieverEvaluator on the evaluation dataset we generated, using the evaluation metrics provided.</p> <ul> <li>hit-rate: the correct answer is present in the top k retrieved results</li> <li>MRR: the reciprocal of the rank at which the first relevant result appears.</li> <li>Precision: the fraction of relevant documents retrieved out of the total number of documents retrieved.</li> <li>Recall: the fraction of relevant documents that were retrieved out of the total number of relevant documents available.</li> <li>AP: the average of precision values at the ranks where relevant documents are retrieved.</li> <li>NDCG: the quality of a ranking based on the positions of relevant documents</li> </ul> </li> </ul> <p>The results demonstrate a clear improvement in retrieval performance of approximately 5% when using the customized PDF data loader</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/result-480.webp 480w,/blog/assets/img/result-800.webp 800w,/blog/assets/img/result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Evaluation of retrieval </div> <p>This experiment highlights that optimizing PDF extraction is crucial and can be just as important as improving embedding models. Please note that this article focuses on PDFs containing only text. If your PDF contains more complex elements, such as images or tables, alternative methods may need to be considered. Recently, the model <a href="https://arxiv.org/html/2407.01449v2">ColPali</a> has gained significant attention for its use of a vision-language model to extract information for retrieval purposes. This approach demonstrates that leveraging modern Vision-Language Models can generate high-quality, contextualized embeddings directly from images of document pages.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="RAG"/><category term="Evaluation"/><summary type="html"><![CDATA[Recently, Retrieval-Augmented Generation (RAG) has emerged as a prominent approach that leverages large language models for building advanced applications. However, in practical industrial settings, the primary bottleneck affecting RAG performance—especially in document retrieval—often lies not in the capabilities of the embedding model, but in the data ingestion pipeline. The process of building a RAG system begins with indexing documents, many of which are in PDF format. This typically involves using PDF parsers to extract text from the document’s pages, a crucial step that significantly impacts the overall retrieval accuracy.]]></summary></entry></feed>