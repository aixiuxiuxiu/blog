<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://aixiuxiuxiu.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aixiuxiuxiu.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-11T10:20:22+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/feed.xml</id><title type="html">blank</title><subtitle>My name is Aixiu An. I document here my learning notes for building robust AI solutions for entreprises. </subtitle><entry><title type="html">How to Encode Long Text Using Large Language Models?</title><link href="https://aixiuxiuxiu.github.io/blog/2024/longcontext/" rel="alternate" type="text/html" title="How to Encode Long Text Using Large Language Models?"/><published>2024-10-18T00:00:00+00:00</published><updated>2024-10-18T00:00:00+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/2024/longcontext</id><content type="html" xml:base="https://aixiuxiuxiu.github.io/blog/2024/longcontext/"><![CDATA[<h2 id="why-long-context-is-so-hard">Why Long Context Is So Hard?</h2> <p>Over the past few years, large language models (LLMs) have made remarkable progress in extending context length limits. For example, BERT-based models typically support up to 512 tokens, while standard GPT-3 models can handle 2,048 tokens. GPT-4 offers two configurations: one with 8,192 tokens and another with an extended window of 32,768 tokens (32K tokens). Recently, Gemmi announced a 2 million-token context window for Gemini 1.5 Pro. However, using models with larger context windows does not necessarily lead to better performance. I hope this blog helps unravel some of the myths about long contexts and explains some current solutions to address this challenge.</p> <p>The limitations of the context window stem from the inherent properties of the transformer architecture itself. Most current LLMs, such as GPT and LLama, rely on the transformer architecture and its self-attention mechanism. This mechanism compares each token in the input sequence with every other token, resulting in a quadratic increase in both memory usage and computational cost as the context length grows. To address the limitations imposed by context length, two main research directions have emerged. The first focuses on training models with larger context windows, aiming to extend these limits. Some research has explored fine-tuning LLMs with longer context inputs (<d-cite key="dubey2024llama"></d-cite>, <d-cite key="tworkowski2024focused"></d-cite>), while others have used position extrapolation or interpolation, building on relative rotary positional embeddings (<d-cite key="su2024roformer"></d-cite>) to extend input lengths beyond the model’s original training limits (<d-cite key="press2021train"></d-cite>, <d-cite key="chen2023extending"></d-cite>).</p> <p>The second approach focuses on improving encoding techniques. Encoding all the information from a multi-page document into a single embedding vector is difficult, if not impossible. Even with a model that has a long context window capable of handling large inputs, encoding everything from multiple pages into one vector may result in the loss of important information. Particularly for retrieval tasks, if you need to extract specific information from a sentence, a large embedding for multi-page text might not effectively capture it. In such cases, chunking long texts into smaller segments while maintaining the dependencies between them offers a more effective approach.</p> <p>This blog will focus on the second direction, discussing available techniques for encoding long contexts in two different tasks: one in building Retrieval-Augmented Generation (RAG) and the other in document classification.</p> <h2 id="encoding-long-contexts-in-rag">Encoding Long Contexts in RAG</h2> <p>Building a RAG system requires integrating internal knowledge bases, which often involves encoding hundreds of pages of documents. Efficiently encoding these documents and facilitating retrieval and generation afterward remain core challenges. Here, I will explain two encoding techniques: naive chunking and late chunking.</p> <h3 id="naive-chunking">Naive chunking</h3> <p>The naive encoding approach (shown on the left side of the image below) involves splitting the text into chunks beforehand and applying an embedding model to each chunk. A common method for generating a single embedding from each chunk is to use mean pooling on the token-level embeddings, where the embeddings of all tokens are averaged.</p> <p>To split the text, we can use a fixed length (see an implementation in the <a href="https://cookbook.openai.com/examples/embedding_long_inputs">OpenAI CookBook</a>), though in some cases, splitting at paragraph or sentence boundaries may better preserve the text’s meaning. This approach has been implemented in Langchain via the function <code class="language-plaintext highlighter-rouge">RecursiveCharacterTextSplitter</code> (see this <a href="https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846">blog</a> for more detailed explanations of this function.)</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/splitter.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Example of chunking with sentence boundaries </div> <p>As a result, naive chunking allows encoding the entire sequence without cutting until the maximum context window is reached. However, because it encodes each chunk independently, it breaks the dependencies between chunks. This means that each chunk is treated as an independent element, without considering the context before or after it.</p> <h3 id="late-chunking">Late chunking</h3> <p>Late chunking, introduced by Jina AI <d-cite key="gunther2024late"></d-cite>, addresses the contextual issue. Their technique involves first encoding the entire document with a long context embedding model to produce embeddings for all tokens in the text (or at least as long as possible). Once the token-level embeddings are generated using the long-context embedding model, the text is divided into chunks, and mean pooling is applied to the tokens in each chunk to create chunk-level embeddings. This ensures that each chunk embedding is informed by the context of the whole document, not just the local text of the chunk.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <figure style="width: 70%; margin: 0 auto; background-color: black; padding: 10px; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/latechuncking-480.webp 480w,/blog/assets/img/latechuncking-800.webp 800w,/blog/assets/img/latechuncking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/latechuncking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="text-white text-center mt-2"> An illustration of the naive chunking strategy (left) and the late chunking strategy (right), from Jina AI <a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/" class="text-white">blog</a> </figcaption> </figure> </div> </div> <p>It is important to note that effective late chunking relies on embedding models with long-context capabilities. In their example, they use <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/">jina-embeddings-v2-base-en</a>, which can handle up to 8,192 tokens—roughly the equivalent of ten standard pages of text. Recently, other embeddings have become available, such as <a href="https://blog.voyageai.com/2024/09/18/voyage-3/">voyage-3</a>, which supports up to 32,000 tokens.</p> <p>To evaluate the effectiveness of late chunking, they tested several retrieval tasks from the <a href="https://github.com/beir-cellar/beir">BeIR benchmark</a>. In all cases, late chunking outperformed the naive approach, particularly for longer documents, where the performance gap between the two methods increased with document length. This demonstrates that late chunking becomes more effective as document length grows.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/jina_length-480.webp 480w,/blog/assets/img/jina_length-800.webp 800w,/blog/assets/img/jina_length-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/jina_length.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="text-black text-center mt-2" style="color: black; width: 100%;"> Late chunking's improvement over naive chunking in retrieval tasks is correlated with the avg. document length, from Jina AI <a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/">blog</a> </figcaption> </figure> </div> </div> <h2 id="encoding-long-contexts-in-document-classification">Encoding Long Contexts in Document Classification</h2> <p>Unlike RAG, which relies on retrieving relevant pieces of information, classification tasks typically require fine-tuning a LLM for the downstream task. In such tasks, a [CLS] token is added at the beginning of the input sequence, and its final embedding, representing the entire sequence, is used for classification by adding a classifier layer on top. Fine-tuning is then used to adjust the weights of a pre-trained transformer model (like BERT) on task-specific labeled data, enabling it to make accurate predictions for that particular task.</p> <p>Here, I would like to illustrate using the EURLEX-57K dataset <d-cite key="chalkidis2019large"></d-cite>, a multi-label classification dataset based on EU legal documents. EURLEX-57K includes 57,000 legislative documents from EUR-LEX, with an average length of 727 words.</p> <table> <thead> <tr> <th>Input(s)</th> <th>Output(s) / Label(s)</th> </tr> </thead> <tbody> <tr> <td><strong>Text</strong>: <small> Commission Regulation (EC) No 1156/2001 of 13 June 2001 fixing the export refunds on white sugar and raw sugar exported in its unaltered state. <br/> THE COMMISSION OF THE EUROPEAN COMMUNITIES Having regard to the Treaty establishing the European Community, Having regard to Council Regulation (EC) No 2038/1999 of 13 September 1999 on the common organisation of the markets in the sugar sector(1), as amended by Commission Regulation (EC) No 1527/2000(2), and in particular point (a) of the second subparagraph of Article 18(5) thereof, <br/> Whereas: (1) Article 18 of Regulation (EC) No 2038/1999 provides that the difference between quotations or prices on the world market for the products listed in Article 1(1)(a) of that Regulation and prices for those products within the Community may be covered by an export refund. (2) Regulation (EC) No 2038/1999 provides that when refunds on white and raw sugar, undenatured and exported in its unaltered state, are being fixed account must be taken of the situation on the Community and world markets in sugar and in particular of the price and cost factors […] </small></td> <td>28 (Trade Policy) <br/> 93 (Beverages and Sugar) <br/> 94 (Foodstuff)</td> </tr> </tbody> </table> <p>Several approaches are available to bypass BERT’s maximum text length limit</p> <ul> <li><strong>Document Truncation</strong>: The simplest approach involves fine-tuning BERT by truncating long documents to the first 512 tokens (this can be done by setting <code class="language-plaintext highlighter-rouge">truncation=True</code> in the tokenizer function).</li> </ul> <p><small><span style="background-color: lightgreen"> Commission Regulation (EC) No 1156/2001 of 13 June 2001 fixing the export refunds on white sugar and raw sugar exported in its unaltered state. <br/> THE COMMISSION OF THE EUROPEAN COMMUNITIES Having regard to the Treaty establishing the European Community, Having regard to Council Regulation (EC) No 2038/1999 of 13 September 1999 on the common organisation of the markets in the sugar sector(1), as amended by Commission Regulation (EC) No 1527/2000(2), and in </span> particular point (a) of the second subparagraph of Article 18(5) thereof, <br/> Whereas: (1) Article 18 of Regulation (EC) No 2038/1999 provides that the difference between quotations or prices on the world market for the products listed in Article 1(1)(a) of that Regulation and prices for those products within the Community may be covered by an export refund. (2) Regulation (EC) No 2038/1999 provides that when refunds on white and raw sugar, undenatured and exported in its unaltered state, are being fixed account must be taken of the situation on the Community and world markets in sugar and in particular of the price and cost factors […]</small></p> <ul> <li><strong>Longformer</strong> <d-cite key="beltagy2020longformer"></d-cite> : It is designed to process longer input sequences using an efficient self-attention mechanism that scales linearly with the input length. Unlike BERT, which can handle up to 512 tokens, Longformer can process up to 4,096 tokens.</li> <li><strong>Hierarchical Encoding</strong> <d-cite key="pappagari2019hierarchical"></d-cite>: It divides long documents into smaller chunks of 200 tokens and uses a Transformer layer over BERT-based chunk representations (I implemented it under the Github folder).</li> <li><strong>Cognize LongTeXts</strong> <d-cite key="ding2020cogltx"></d-cite>: In this model, two BERT (or RoBERTa) models are jointly trained to select key sentences from long documents for various tasks including text classification. The underlying idea that a few key sentences are sufficient for a given task has been explored for question answering.</li> </ul> <p>However, in this recent paper <d-cite key="park2022efficient"></d-cite>, they evaluate different models and show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets.</p> <p>As mentioned above, the context limitation arises from the transformer architecture. Some research has focused on developing new architectures, such as the State Space Model (SSM). However, there is still limited understanding of how these models can improve tasks like RAG or classification.</p> <h4 id="acknowledgment">Acknowledgment</h4>]]></content><author><name></name></author><category term="RAG"/><category term="Classification"/><summary type="html"><![CDATA[This blog explores methods for encoding long contexts using large language models, focusing on techniques for Retrieval-Augmented Generation (RAG) and document classification.]]></summary></entry><entry><title type="html">Optimizing PDF Extraction for Building a Robust RAG</title><link href="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/" rel="alternate" type="text/html" title="Optimizing PDF Extraction for Building a Robust RAG"/><published>2024-10-17T00:32:13+00:00</published><updated>2024-10-17T00:32:13+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/2024/pdfextractor</id><content type="html" xml:base="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/"><![CDATA[<p>Recently, Retrieval-Augmented Generation (RAG) has emerged as a prominent approach that leverages large language models for building advanced applications. However, in practical industrial settings, the primary bottleneck affecting RAG performance—especially in document retrieval—often lies not in the capabilities of the embedding model, but in the data ingestion pipeline. The process of building a RAG system begins with indexing documents, many of which are in PDF format. This typically involves using PDF parsers to extract text from the document’s pages, a crucial step that significantly impacts the overall retrieval accuracy.</p> <p>Extracting text from PDFs are challenging in many aspects (can also read <a href="https://pypdf.readthedocs.io/en/stable/user/extract-text.html">this</a>):</p> <ul> <li><strong>Complex and variable structures</strong>: PDFs are designed for visual presentation, not structured text extraction, leading to fragmented or misaligned text.</li> <li><strong>Layout complexity</strong>: PDFs often contain multi-column formats, tables, and embedded images, making it difficult to maintain a logical reading order.</li> <li><strong>Inconsistent text encoding</strong>: Different fonts, character encodings, or special symbols can lead to extraction errors, such as missing or garbled text.</li> <li><strong>Discontinuous text chunks</strong>: Text might be broken into fragments that need to be reassembled into meaningful sequences.</li> </ul> <p>There are various PDF extraction tools available on the market. Some companies provide paid solutions with advanced features, while several open-source Python packages, such as PyPDF and PDFPlumber, are also available. In this blog, I will present an experiment comparing the impact of different PDF extraction methods on retrieval performance. The results show that high-quality PDF extraction can significantly improve retrieval. (The code is available on <a href="https://github.com/aixiuxiuxiu/pdf-extraction-blog/tree/main">Github</a>)</p> <h2 id="pdf-extraction">PDF extraction</h2> <p>The following example is a PDF excerpt from the <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a>. The PDF page presents a high level of complexity, with text that is discontinuously arranged and interspersed with numerous footnotes.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 90%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/pdf_example-480.webp 480w,/blog/assets/img/pdf_example-800.webp 800w,/blog/assets/img/pdf_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/pdf_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here I use <a href="https://github.com/jsvine/pdfplumber"><code class="language-plaintext highlighter-rouge">pdfplumber</code></a>, a library built on top of <a href="https://github.com/goulu/pdfminer"><code class="language-plaintext highlighter-rouge">pdfminer.six</code></a>, that offers a wide range of customizable features for extracting text from PDFs. This package allows the extraction of pages and text while preserving the original layout. Additionally, it can parse various character properties, such as page number, text, and coordinates. For instance, the <code class="language-plaintext highlighter-rouge">.crop()</code> method can be used to crop a page into a specific bounding box: <code class="language-plaintext highlighter-rouge">.crop((x0, top, x1, bottom), relative=False, strict=True)</code>.</p> <p>I built two data loaders. The first one, called <code class="language-plaintext highlighter-rouge">PDFLoader</code>, extracts sequences directly using <code class="language-plaintext highlighter-rouge">pdfplumber</code> without additional postprocessing. In this approach, the raw extraction from the PDF example above appears unstructured and unnatural, as it reads lines horizontally, even when they are split into two blocks.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber_old.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Illustraction of PDFLoader result </div> <p>The other is called <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code>, which uses <code class="language-plaintext highlighter-rouge">pdfplumber</code> to extract text from different boxes and combine them together. Here, I extract text from two distinct boxes: one for the left side and one for the right side of the page. I use the <code class="language-plaintext highlighter-rouge">x0</code> of the word <strong>Art</strong> as the <code class="language-plaintext highlighter-rouge">x0</code> for the right box and the same value as the <code class="language-plaintext highlighter-rouge">x1</code> for the left box. This makes the extracted sequences more natural.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Illustraction of Custom PDFLoader result </div> <h2 id="evaluation">Evaluation</h2> <p>This section evaluates retrieval quality by comparing the two methods of PDF extraction: <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code> and <code class="language-plaintext highlighter-rouge">PDFLoader</code>, using the <code class="language-plaintext highlighter-rouge">RetrieverEvaluator</code> module provided in <code class="language-plaintext highlighter-rouge">LLAMAIndex</code>.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 65%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/pdf-flow-480.webp 480w,/blog/assets/img/pdf-flow-800.webp 800w,/blog/assets/img/pdf-flow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/pdf-flow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>First, I split the extracted text from the PDF file <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a> (about 350 pages), into small chunks of 1024 characters, with a chunk overlap of 200.</li> <li>Second, I created the evaluation dataset using the <code class="language-plaintext highlighter-rouge">generate_question_context_pairs</code> function. This function can automatically generate a set of (question, context) pairs using LLMs. I generated two questions from each context chunk with GPT-4-mini, resulting in a total number of 450 queries.</li> <li>I built a retriever using the built-in <code class="language-plaintext highlighter-rouge">VectorStoreIndex</code> function in <code class="language-plaintext highlighter-rouge">LLAMAIndex</code>, and performed retrieval using the top-k similarity method.</li> <li>Finally, I ran the <code class="language-plaintext highlighter-rouge">RetrieverEvaluator</code> on the evaluation dataset we generated, using the provided evaluation metrics. <ul> <li>hit-rate: the correct answer is present in the top k retrieved results</li> <li>MRR: the reciprocal of the rank at which the first relevant result appears.</li> <li>Precision: the fraction of relevant documents retrieved out of the total number of documents retrieved.</li> <li>Recall: the fraction of relevant documents that were retrieved out of the total number of relevant documents available.</li> <li>AP: the average of precision values at the ranks where relevant documents are retrieved.</li> <li>NDCG: the quality of a ranking based on the positions of relevant documents</li> </ul> </li> </ul> <p>The results demonstrate a clear improvement in retrieval performance of approximately 5% when using the <code class="language-plaintext highlighter-rouge">custom PDFLoader</code>.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 75%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/result-480.webp 480w,/blog/assets/img/result-800.webp 800w,/blog/assets/img/result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This experiment highlights that optimizing PDF extraction is crucial, and can be just as important as improving embedding models. Please note that this article focuses on PDFs containing only text. If your PDF contains more complex elements, such as images or tables, alternative methods may need to be considered. Recently, there has been a spike in interest in the new model <a href="https://arxiv.org/pdf/2407.01449">ColPali</a> for its use of a vision-language model to extract information for retrieval purposes. This approach demonstrates that when a document contains rich structures, such as tables and figures, leveraging modern Vision-Language Models can generate high-quality, contextualized embeddings directly from images of the document pages.</p>]]></content><author><name>Aixiu An</name></author><category term="sample-posts"/><category term="RAG"/><category term="Evaluation"/><summary type="html"><![CDATA[This blog presents an experiment illustrating how high-quality PDF extraction can significantly improve RAG performance. It highlights that improving PDF extraction may be just as important as enhancing embedding models.]]></summary></entry></feed>