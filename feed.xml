<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://aixiuxiuxiu.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aixiuxiuxiu.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-10T14:35:11+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/feed.xml</id><title type="html">AiShow</title><subtitle>My name is . I document here my learning notes for building robust AI solutions for entreprises. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How LLMs handle long context?</title><link href="https://aixiuxiuxiu.github.io/blog/2024/longcontext/" rel="alternate" type="text/html" title="How LLMs handle long context?"/><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/2024/longcontext</id><content type="html" xml:base="https://aixiuxiuxiu.github.io/blog/2024/longcontext/"><![CDATA[<h2 id="why-long-context-is-so-hard">Why long context is so hard?</h2> <p>Long contexts present several challenges for large language models (LLMs), as most current models have limited context windows. For instance, BERT-based models typically have a window of 512 tokens– if a sequence exceeds 512 tokens, only part of it is encoded. In contrast, standard GPT-3 models handle around 2,048 tokens, while GPT-4 offers two variants: one with 8,192 tokens and another with an extended window of 32,768 tokens (32K tokens). However, many applications involving LLMs require handling documents that far exceed these limits. For example, building a retrieval-augmented generation (RAG) system to integrate internal documents often involves encoding multi-page documents. Similarly, chat applications may need to integrate previous conversations that span several pages.</p> <table> <thead> <tr> <th>Model</th> <th>Context length</th> <th>Number of English pages*</th> </tr> </thead> <tbody> <tr> <td>GPT 3.5</td> <td>4,096</td> <td>6</td> </tr> <tr> <td>GPT 4</td> <td>8,192</td> <td>12</td> </tr> <tr> <td>GPT 4-32k</td> <td>32,768</td> <td>49</td> </tr> <tr> <td>Llama 1</td> <td>2,048</td> <td>3</td> </tr> <tr> <td>Llama 2</td> <td>4,096</td> <td>6</td> </tr> </tbody> </table> <p><em>Context Length Comparison (</em>Assuming 500 words per page.)</p> <p>In addressing this challenge, two main research directions have emerged. The first is to develop models with longer context windows, as illustrated by the table showing their evolution. However, this is challenging because most LLMs, such as GPT and BERT, are based on the Transformer architecture, which uses a self-attention mechanism. This mechanism compares each token in the input sequence with every other token, leading to quadratic complexity in memory usage and computational cost.</p> <p>The second approach involves improving encoding techniques. Encoding all the information from a multi-page document into a single embedding vector is difficult, if not impossible. Although we have a model with a long context window, encoding everything into one vector can lead to information loss. Alternatively, chunking long texts into smaller segments, while still accounting for dependencies between them, provides another viable approach.</p> <h2 id="long-context-in-training">Long context in training</h2> <p>While most closed-source models provide limited information on this topic, the LLama report <d-cite key="dubey2024llama"></d-cite> elaborates on it, explaining that handling long contexts typically occurs during the later stages of pretraining:</p> <blockquote> <p>In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves “needle in a haystack” tasks up to that length. In Llama 3 405B pre-training, we increased context length gradually in six stages, starting from the original 8K context window and ending in the final 128K context window. This long-context pre-training stage was performed using approximately 800B training tokens.</p> </blockquote> <h2 id="long-context-in-encoding">Long context in encoding</h2> <p>Recently, many progress has made to encode the long context efficiently:</p> <ul> <li>Navive chuncking: The naive encoding approach (as seen on the left side of the image below) involves using sentences, paragraphs, or maximum length limits to split the text a priori. Afterward, an embedding model is repetitively applied to these resulting chunks. To generate a single embedding for each chunk, many embedding models use mean pooling on these token-level embeddings to output a single embedding vector. See an example from <a href="https://cookbook.openai.com/examples/embedding_long_inputs">OpenAI CookBook</a></li> <li>Late chuncking Jina AI <d-cite key="gunther2024late"></d-cite>: first applies the transformer layer of the embedding model to the entire text or as much of it as possible. This generates a sequence of vector representations for each token that encompasses textual information from the entire text. Subsequently, mean pooling is applied to each chunk of this sequence of token vectors, yielding embeddings for each chunk that consider the entire text’s context. Unlike the naive encoding approach, which generates independent and identically distributed (i.i.d.) chunk embeddings, late chunking creates a set of chunk embeddings where each one is “conditioned on” the previous ones, thereby encoding more contextual information for each chunk.</li> </ul> <div class="row mt-3" style="background-color: black;"> <div class="col-sm mt-3 mt-md-0"> <figure style="width: 90%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/latechuncking-480.webp 480w,/blog/assets/img/latechuncking-800.webp 800w,/blog/assets/img/latechuncking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/latechuncking.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="text-white text-center mt-2"> An illustration of the naive chunking strategy (left) and the late chunking strategy (right), from Jina AI <a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/" class="text-white">blog</a> </figcaption> </figure> </div> </div> <ul> <li>ColBERT’s Late Interaction <d-cite key="santhanam2021colbertv2"></d-cite></li> </ul> <h2 id="long-context-in-evaluation">Long context in evaluation</h2> <d-cite key="dubois2024length"></d-cite>]]></content><author><name></name></author><category term="LLM"/><category term="evaluation"/><summary type="html"><![CDATA[Why long context is so hard?]]></summary></entry><entry><title type="html">Optimizing PDF Extraction for Building a Robust RAG</title><link href="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/" rel="alternate" type="text/html" title="Optimizing PDF Extraction for Building a Robust RAG"/><published>2024-09-11T00:32:13+00:00</published><updated>2024-09-11T00:32:13+00:00</updated><id>https://aixiuxiuxiu.github.io/blog/2024/pdfextractor</id><content type="html" xml:base="https://aixiuxiuxiu.github.io/blog/2024/pdfextractor/"><![CDATA[<p>Recently, Retrieval-Augmented Generation (RAG) has emerged as a prominent approach that leverages large language models for building advanced applications. However, in practical industrial settings, the primary bottleneck affecting RAG performance—especially in document retrieval—often lies not in the capabilities of the embedding model, but in the data ingestion pipeline. The process of building a RAG system begins with indexing documents, many of which are in PDF format. This typically involves using PDF parsers to extract text from the document’s pages, a crucial step that significantly impacts the overall retrieval accuracy.</p> <p>Extracting text from PDFs are challenging in many aspects (can also read <a href="https://pypdf.readthedocs.io/en/stable/user/extract-text.html">this</a>):</p> <ul> <li><strong>Complex and variable structures</strong>: PDFs are designed for visual presentation, not structured text extraction, leading to fragmented or misaligned text.</li> <li><strong>Layout complexity</strong>: PDFs often contain multi-column formats, tables, and embedded images, making it difficult to maintain a logical reading order.</li> <li><strong>Inconsistent text encoding</strong>: Different fonts, character encodings, or special symbols can lead to extraction errors, such as missing or garbled text.</li> <li><strong>Discontinuous text chunks</strong>: Text might be broken into fragments that need to be reassembled into meaningful sequences.</li> </ul> <p>There are various PDF extraction tools available on the market. Some companies provide paid solutions with advanced features, while several open-source Python packages, such as PyPDF and PDFPlumber, are also available. In this blog, I will present an experiment comparing the impact of different PDF extraction methods on retrieval performance. The results show that high-quality PDF extraction can significantly improve retrieval. (The code is available on <a href="https://github.com/aixiuxiuxiu/pdf-extraction-blog/tree/main">Github</a>)</p> <h2 id="pdf-extraction">PDF extraction</h2> <p>The following example is a PDF excerpt from the <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a>. The PDF page presents a high level of complexity, with text that is discontinuously arranged and interspersed with numerous footnotes.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 90%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/pdf_example-480.webp 480w,/blog/assets/img/pdf_example-800.webp 800w,/blog/assets/img/pdf_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/pdf_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here I use <a href="https://github.com/jsvine/pdfplumber"><code class="language-plaintext highlighter-rouge">pdfplumber</code></a>, a library built on top of <a href="https://github.com/goulu/pdfminer"><code class="language-plaintext highlighter-rouge">pdfminer.six</code></a>, that offers a wide range of customizable features for extracting text from PDFs. This package allows the extraction of pages and text while preserving the original layout. Additionally, it can parse various character properties, such as page number, text, and coordinates. For instance, the <code class="language-plaintext highlighter-rouge">.crop()</code> method can be used to crop a page into a specific bounding box: <code class="language-plaintext highlighter-rouge">.crop((x0, top, x1, bottom), relative=False, strict=True)</code>.</p> <p>I built two data loaders. The first one, called <code class="language-plaintext highlighter-rouge">PDFLoader</code>, extracts sequences directly using <code class="language-plaintext highlighter-rouge">pdfplumber</code> without additional postprocessing. In this approach, the raw extraction from the PDF example above appears unstructured and unnatural, as it reads lines horizontally, even when they are split into two blocks.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber_old.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Illustraction of PDFLoader result </div> <p>The other is called <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code>, which uses <code class="language-plaintext highlighter-rouge">pdfplumber</code> to extract text from different boxes and combine them together. Here, I extract text from two distinct boxes: one for the left side and one for the right side of the page. I use the <code class="language-plaintext highlighter-rouge">x0</code> of the word <strong>Art</strong> as the <code class="language-plaintext highlighter-rouge">x0</code> for the right box and the same value as the <code class="language-plaintext highlighter-rouge">x1</code> for the left box. This makes the extracted sequences more natural.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/pdfplumber.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Illustraction of Custom PDFLoader result </div> <h2 id="evaluation">Evaluation</h2> <p>This section evaluates retrieval quality by comparing the two methods of PDF extraction: <code class="language-plaintext highlighter-rouge">Custom PDFLoader</code> and <code class="language-plaintext highlighter-rouge">PDFLoader</code>, using the <code class="language-plaintext highlighter-rouge">RetrieverEvaluator</code> module provided in <code class="language-plaintext highlighter-rouge">LLAMAIndex</code>.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 65%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/pdf-flow-480.webp 480w,/blog/assets/img/pdf-flow-800.webp 800w,/blog/assets/img/pdf-flow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/pdf-flow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>First, I split the extracted text from the PDF file <a href="https://www.fedlex.admin.ch/eli/cc/24/233_245_233/en">Swiss Civil Code</a> (about 350 pages), into small chunks of 1024 characters, with a chunk overlap of 200.</li> <li>Second, I created the evaluation dataset using the <code class="language-plaintext highlighter-rouge">generate_question_context_pairs</code> function. This function can automatically generate a set of (question, context) pairs using LLMs. I generated two questions from each context chunk with GPT-4-mini, resulting in a total number of 450 queries.</li> <li>I built a retriever using the built-in <code class="language-plaintext highlighter-rouge">VectorStoreIndex</code> function in <code class="language-plaintext highlighter-rouge">LLAMAIndex</code>, and performed retrieval using the top-k similarity method.</li> <li>Finally, I ran the <code class="language-plaintext highlighter-rouge">RetrieverEvaluator</code> on the evaluation dataset we generated, using the provided evaluation metrics. <ul> <li>hit-rate: the correct answer is present in the top k retrieved results</li> <li>MRR: the reciprocal of the rank at which the first relevant result appears.</li> <li>Precision: the fraction of relevant documents retrieved out of the total number of documents retrieved.</li> <li>Recall: the fraction of relevant documents that were retrieved out of the total number of relevant documents available.</li> <li>AP: the average of precision values at the ranks where relevant documents are retrieved.</li> <li>NDCG: the quality of a ranking based on the positions of relevant documents</li> </ul> </li> </ul> <p>The results demonstrate a clear improvement in retrieval performance of approximately 5% when using the <code class="language-plaintext highlighter-rouge">custom PDFLoader</code>.</p> <div class="row justify-content-center mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="width: 75%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/result-480.webp 480w,/blog/assets/img/result-800.webp 800w,/blog/assets/img/result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This experiment highlights that optimizing PDF extraction is crucial, and can be just as important as improving embedding models. Please note that this article focuses on PDFs containing only text. If your PDF contains more complex elements, such as images or tables, alternative methods may need to be considered. Recently, there has been a spike in interest in the new model <a href="https://arxiv.org/pdf/2407.01449">ColPali</a> for its use of a vision-language model to extract information for retrieval purposes. This approach demonstrates that when a document contains rich structures, such as tables and figures, leveraging modern Vision-Language Models can generate high-quality, contextualized embeddings directly from images of the document pages.</p>]]></content><author><name>Aixiu An</name></author><category term="sample-posts"/><category term="RAG"/><category term="Evaluation"/><summary type="html"><![CDATA[Recently, Retrieval-Augmented Generation (RAG) has emerged as a prominent approach that leverages large language models for building advanced applications. However, in practical industrial settings, the primary bottleneck affecting RAG performance—especially in document retrieval—often lies not in the capabilities of the embedding model, but in the data ingestion pipeline. The process of building a RAG system begins with indexing documents, many of which are in PDF format. This typically involves using PDF parsers to extract text from the document’s pages, a crucial step that significantly impacts the overall retrieval accuracy.]]></summary></entry></feed>