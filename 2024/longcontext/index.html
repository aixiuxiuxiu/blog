<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Encode Long Text Using Large Language Models? | AiShow </title> <meta name="author" content="Aixiu An"> <meta name="description" content="This blog explores methods for encoding long contexts using large language models, focusing on techniques for Retrieval-Augmented Generation (RAG) and document classification."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aixiuxiuxiu.github.io/blog/2024/longcontext/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How to Encode Long Text Using Large Language Models?",
            "description": "This blog explores methods for encoding long contexts using large language models, focusing on techniques for Retrieval-Augmented Generation (RAG) and document classification.",
            "published": "October 18, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/"> AiShow </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How to Encode Long Text Using Large Language Models?</h1> <p>This blog explores methods for encoding long contexts using large language models, focusing on techniques for Retrieval-Augmented Generation (RAG) and document classification.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#why-long-context-is-so-hard">Why Long Context Is So Hard?</a> </div> <div> <a href="#encoding-long-contexts-in-retrieval-augmented-generation-rag">Encoding Long Contexts in Retrieval-Augmented Generation (RAG)</a> </div> <ul> <li> <a href="#naive-chunking">Naive chunking</a> </li> <li> <a href="#late-chunking">Late chunking</a> </li> </ul> <div> <a href="#encoding-long-contexts-in-document-classification">Encoding Long Contexts in Document Classification</a> </div> <ul> <li> <a href="#document-truncation">Document Truncation</a> </li> <li> <a href="#longformer">Longformer</a> </li> <li> <a href="#hierarchical-encoding">Hierarchical Encoding</a> </li> <li> <a href="#cognize-longtexts">Cognize LongTeXts</a> </li> </ul> </nav> </d-contents> <h2 id="why-long-context-is-so-hard">Why long context is so hard?</h2> <p>Over the past few years, large language models (LLMs) have made remarkable progress in extending context length limits. For example, BERT-based models typically support up to 512 tokens, while standard GPT-3 models can handle 2,048 tokens. GPT-4 offers two configurations: one with 8,192 tokens and another with an extended window of 32,768 tokens (32K tokens). Recently, Gemmi announced a 2 million-token context window for Gemini 1.5 Pro. However, using models with larger context windows does not necessarily lead to better performance. I hope this blog helps unravel some of the myths about long contexts and explains some current solutions to address this challenge.</p> <p>The limits of context window stems from the limits of transfomer architecture itself. Most current LLMs, such as GPT and LLama, rely on the transformer architecture and its self-attention mechanism. This mechanism compares each token in the input sequence with every other token, resulting in quadratic complexity in both memory usage and computational cost. To overcome the limitations imposed by context length, two primary research directions have emerged. The first one seeks to extend the limits of context window. Some research has explored fine-tuning LLMs with longer context inputs (<d-cite key="dubey2024llama"></d-cite>, <d-cite key="tworkowski2024focused"></d-cite>), while others have used position extrapolation or interpolation, building on relative rotary positional embeddings (<d-cite key="su2024roformer"></d-cite>) to extend input lengths beyond the model’s original training limits (<d-cite key="press2021train"></d-cite>, <d-cite key="chen2023extending"></d-cite>).</p> <p>The second direction focuses on improving encoding techniques. Encoding all the information from a multi-page document into a single embedding vector is difficult, if not impossible. However, even with a model that has a long context window capable of doing so, trying to encode everything from multiple pages into one vector may result in the loss of important information. Particularly for retrieval tasks, if you need to extract specific information within a sentence, a large embedding for multi-page text might not effectively capture it. Alternatively, chunking long texts into smaller segments while maintaining the dependencies between them opens another direction.</p> <p>This blog will focus on the second direction, discussing available techniques for encoding long contexts in two different tasks: one in building Retrieval-Augmented Generation (RAG) and the other in document classification.</p> <h2 id="encoding-long-contexts-in-rag">Encoding Long Contexts in RAG</h2> <p>Building a RAG system requires integrating internal knowledge bases, which often involves encoding hundreds of pages of documents. Efficiently encoding these documents and facilitating retrieval and generation afterward remain core challenges. Here, I will explain two encoding techniques: naive chunking and late chunking.</p> <h3 id="naive-chunking">Naive chunking</h3> <p>The naive encoding approach (shown on the left side of the image below) involves splitting the text into chunks beforehand and applying an embedding model to each chunk. A common method for generating a single embedding from each chunk is to use mean pooling on the token-level embeddings, where the embeddings of all tokens are averaged.</p> <p>To split the text, we can use a fixed length (see an implementation in the <a href="https://cookbook.openai.com/examples/embedding_long_inputs" rel="external nofollow noopener" target="_blank">OpenAI CookBook</a>), though in some cases, splitting at paragraph or sentence boundaries may better preserve the text’s meaning. This approach has been implemented in Langchain via the function <code class="language-plaintext highlighter-rouge">RecursiveCharacterTextSplitter</code> (see this <a href="https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846" rel="external nofollow noopener" target="_blank">blog</a> for more detailed explanations of this function.)</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/blog/assets/jupyter/splitter.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <div class="caption"> Example of chunking with sentence boundaries </div> <p>As a result, naive chunking allows encoding the entire sequence without cutting until the maximum context window is reached. However, because it encodes each chunk independently, it breaks the dependencies between chunks. This means that each chunk is treated as an independent element, without considering the context before or after it.</p> <h3 id="late-chunking">Late chunking</h3> <p>Late chunking, introduced by Jina AI <d-cite key="gunther2024late"></d-cite>, addresses the contextual issue. Their technique involves first encoding the entire document with a long context embedding model to produce embeddings for all tokens in the text (or at least as long as possible). Once the token-level embeddings are generated using the long-context embedding model, the text is divided into chunks, and mean pooling is applied to the tokens in each chunk to create chunk-level embeddings. This ensures that each chunk embedding is informed by the context of the whole document, not just the local text of the chunk.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <figure style="width: 70%; margin: 0 auto; background-color: black; padding: 10px; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/latechuncking-480.webp 480w,/blog/assets/img/latechuncking-800.webp 800w,/blog/assets/img/latechuncking-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/assets/img/latechuncking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="text-white text-center mt-2"> An illustration of the naive chunking strategy (left) and the late chunking strategy (right), from Jina AI <a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/" class="text-white" rel="external nofollow noopener" target="_blank">blog</a> </figcaption> </figure> </div> </div> <p>It is important to note that effective late chunking relies on embedding models with long-context capabilities. In their example, they use <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/" rel="external nofollow noopener" target="_blank">jina-embeddings-v2-base-en</a>, which can handle up to 8,192 tokens—roughly the equivalent of ten standard pages of text. Recently, other embeddings have become available, such as <a href="https://blog.voyageai.com/2024/09/18/voyage-3/" rel="external nofollow noopener" target="_blank">voyage-3</a>, which supports up to 32,000 tokens.</p> <p>To evaluate the effectiveness of late chunking, they tested several retrieval tasks from the <a href="https://github.com/beir-cellar/beir" rel="external nofollow noopener" target="_blank">BeIR benchmark</a>. In all cases, late chunking outperformed the naive approach, particularly for longer documents, where the performance gap between the two methods increased with document length. This demonstrates that late chunking becomes more effective as document length grows.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure style="width: 60%; margin: 0 auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/jina_length-480.webp 480w,/blog/assets/img/jina_length-800.webp 800w,/blog/assets/img/jina_length-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/assets/img/jina_length.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="text-black text-center mt-2" style="color: black; width: 130%;"> Late chunking's improvement over naive chunking in retrieval tasks is correlated with the avg. document length, from Jina AI <a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/" rel="external nofollow noopener" target="_blank">blog</a> </figcaption> </figure> </div> </div> <h2 id="encoding-long-contexts-in-document-classification">Encoding Long Contexts in Document Classification</h2> <p>Unlike RAG, which relies on retrieving relevant pieces of information, the model needs to be fine-tuned on labeled data for specific classification tasks. In classification, after obtaining embeddings from the chunks, you can add a classifier layer (typically a fully connected layer) on top of the aggregated global embedding and fine-tune the entire model on your classification dataset.</p> <p>If you’re working with a BERT-based classification model where the context window is 512 tokens, but your document has more than 5,000 tokens. what you could do in such case? Several ways are available to handle this:</p> <ul> <li> <strong>Document Truncation</strong>: The simplest approach involves fine-tuning BERT by truncating long documents to the first 512 tokens (this can be done by setting <code class="language-plaintext highlighter-rouge">truncation=True</code> in the tokenizer function).</li> <li> <strong>Longformer</strong> <d-cite key="beltagy2020longformer"></d-cite> : It is designed to process longer input sequences using an efficient self-attention mechanism that scales linearly with the input length. Unlike BERT, which can handle up to 512 tokens, Longformer can process up to 4,096 tokens.</li> <li> <strong>Hierarchical Encoding</strong> <d-cite key="pappagari2019hierarchical"></d-cite>: It divides long documents into smaller chunks of 200 tokens and uses a Transformer layer over BERT-based chunk representations (I implemented it under the Github folder).</li> <li> <strong>Cognize LongTeXts</strong> <d-cite key="ding2020cogltx"></d-cite>: In this model, two BERT (or RoBERTa) models are jointly trained to select key sentences from long documents for various tasks including text classification. The underlying idea that a few key sentences are sufficient for a given task has been explored for question answering.</li> </ul> <p>However, in this recent paper <d-cite key="park2022efficient"></d-cite>, they evaluate different models and show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets.</p> <p>As mentioned above, the context limitation arises from the transformer architecture. Some research has focused on developing new architectures, such as the State Space Model (SSM). However, there is still limited understanding of how these models can improve tasks like RAG or classification.</p> <h4 id="acknowledgment">Acknowledgment</h4> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2024-10-6-longcontext.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Aixiu An. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/index.html"}},{id:"post-how-to-encode-long-text-using-large-language-models",title:"How to Encode Long Text Using Large Language Models?",description:"This blog explores methods for encoding long contexts using large language models, focusing on techniques for Retrieval-Augmented Generation (RAG) and document classification.",section:"Posts",handler:()=>{window.location.href="/blog/2024/longcontext/"}},{id:"post-optimizing-pdf-extraction-for-building-a-robust-rag",title:"Optimizing PDF Extraction for Building a Robust RAG",description:"This blog presents an experiment illustrating how high-quality PDF extraction can significantly improve RAG performance. It highlights that improving PDF extraction may be just as important as enhancing embedding models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/pdfextractor/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/blog/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>